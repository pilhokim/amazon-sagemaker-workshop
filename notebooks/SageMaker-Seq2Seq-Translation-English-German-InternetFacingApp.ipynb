{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation English-German Example Using SageMaker Seq2Seq\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Download dataset and preprocess](#Download-dataset-and-preprocess)\n",
    "3. [Training the Machine Translation model](#Training-the-Machine-Translation-model)\n",
    "4. [Inference](#Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our Machine Translation end-to-end example! In this demo, we will use a pre-trained English-German translation model and will deploy it for an internet-facing App. This notebook will take about 12-15 minutes to complete.\n",
    "\n",
    "SageMaker Seq2Seq algorithm is built on top of [Sockeye](https://github.com/awslabs/sockeye), a sequence-to-sequence framework for Neural Machine Translation based on MXNet. SageMaker Seq2Seq implements state-of-the-art encoder-decoder architectures which can also be used for tasks like Abstractive Summarization in addition to Machine Translation.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. **This should be within the same region as the Notebook Instance, training, and hosting.**\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp in the cell below with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "# S3 bucket and prefix\n",
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/<your_s3_prefix_here>'  # E.g.'sagemaker/seq2seq/eng-german'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Python libraries we'll need for the remainder of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# For plotting attention matrix later on\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(bucket, prefix, channel, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = prefix + \"/\" + channel + '/' + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker Seq2Seq container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/seq2seq:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/seq2seq:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/seq2seq:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/seq2seq:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/seq2seq:latest'}\n",
    "container = containers[region_name]\n",
    "print('Using SageMaker Seq2Seq container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means translating sentence(s) from English to German.\n",
    "This section involves several steps,\n",
    "- Create model - Create a model using the artifact (model.tar.gz) produced by training\n",
    "- Create Endpoint Configuration - Create a configuration defining an endpoint, using the above model\n",
    "- Create Endpoint - Use the configuration to create an inference endpoint.\n",
    "- Perform Inference - Perform inference on some input data using the endpoint.\n",
    "\n",
    "### Create model\n",
    "We now create a SageMaker Model from the training output. Using the model, we can then create an Endpoint Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_pretrained_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pretrained model\n",
    "#### Please uncomment and run the cell below if you want to use a pretrained model, as training might take several hours/days to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  312M  100  312M    0     0  14.1M      0  0:00:22  0:00:22 --:--:-- 15.2M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1066k  100 1066k    0     0  1066k      0  0:00:01  0:00:01 --:--:--  623k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1180k  100 1180k    0     0  1180k      0  0:00:01  0:00:01 --:--:--  652k\n"
     ]
    }
   ],
   "source": [
    "use_pretrained_model = True\n",
    "model_name = \"pretrained-en-de-model\"\n",
    "!curl https://s3-us-west-2.amazonaws.com/gsaur-seq2seq-data/seq2seq/eng-german/full-nb-translation-eng-german-p2-16x-2017-11-24-22-25-53/output/model.tar.gz > model.tar.gz\n",
    "!curl https://s3-us-west-2.amazonaws.com/gsaur-seq2seq-data/seq2seq/eng-german/full-nb-translation-eng-german-p2-16x-2017-11-24-22-25-53/output/vocab.src.json > vocab.src.json\n",
    "!curl https://s3-us-west-2.amazonaws.com/gsaur-seq2seq-data/seq2seq/eng-german/full-nb-translation-eng-german-p2-16x-2017-11-24-22-25-53/output/vocab.trg.json > vocab.trg.json\n",
    "upload_to_s3(bucket, prefix, 'pretrained_model', 'model.tar.gz')\n",
    "model_data = \"s3://{}/{}/pretrained_model/model.tar.gz\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained-en-de-model\n",
      "s3://pilho-sagemaker-ai-workshop/sagemaker/pilho-sagemaker-ai-workshop/pretrained_model/model.tar.gz\n",
      "arn:aws:sagemaker:us-east-1:082256166551:model/pretrained-en-de-model\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 304 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sage = boto3.client('sagemaker')\n",
    "\n",
    "if not use_pretrained_model:\n",
    "    info = sage.describe_training_job(TrainingJobName=job_name)\n",
    "    model_name=job_name\n",
    "    model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print(model_name)\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = sage.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "Use the model to create an endpoint configuration. The endpoint configuration also contains information about the type and number of EC2 instances to use when hosting the model.\n",
    "\n",
    "Since SageMaker Seq2Seq is based on Neural Nets, we could use an ml.p2.xlarge (GPU) instance, but for this example we will use a free tier eligible ml.m4.xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqEndpointConfig-2018-03-24-08-35-50\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:082256166551:endpoint-config/seq2seqendpointconfig-2018-03-24-08-35-50\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'Seq2SeqEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sage.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Lastly, we create the endpoint that serves up model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 10-15 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqEndpoint-2018-03-24-08-35-53\n",
      "arn:aws:sagemaker:us-east-1:082256166551:endpoint/seq2seqendpoint-2018-03-24-08-35-53\n",
      "Status: Creating\n",
      "Endpoint creation ended with EndpointStatus = InService\n",
      "CPU times: user 92 ms, sys: 4 ms, total: 96 ms\n",
      "Wall time: 11min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'Seq2SeqEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sage.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "# wait until the status has changed\n",
    "sage.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message,\n",
    "> Endpoint creation ended with EndpointStatus = InService\n",
    "\n",
    "then congratulations! You now have a functioning inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console.  \n",
    "\n",
    "We will finally create a runtime object from which we can invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runtime = boto3.client(service_name='runtime.sagemaker') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using JSON format for inference (Suggested for a single or small number of data instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that you don't have to convert string to text using the vocabulary mapping for inference using JSON mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'target': 'Sie sind so gut !'}, {'target': 'können Sie ein Auto fahren ?'}, {'target': 'ich will einen Film besuchen .'}]}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"you are so good !\",\n",
    "             \"can you drive a car ?\",\n",
    "             \"i want to watch a movie .\"\n",
    "            ]\n",
    "\n",
    "payload = {\"instances\" : []}\n",
    "for sent in sentences:\n",
    "    payload[\"instances\"].append({\"data\" : sent})\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=json.dumps(payload))\n",
    "\n",
    "response = response[\"Body\"].read().decode(\"utf-8\")\n",
    "response = json.loads(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the Attention Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing `\"attention_matrix\":\"true\"` in `configuration` of the data instance will return the attention matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: can you drive a car ? \n",
      "Target: können Sie ein Auto fahren ?\n"
     ]
    }
   ],
   "source": [
    "sentence = 'can you drive a car ?'\n",
    "\n",
    "payload = {\"instances\" : [{\n",
    "                            \"data\" : sentence,\n",
    "                            \"configuration\" : {\"attention_matrix\":\"true\"}\n",
    "                          }\n",
    "                         ]}\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=json.dumps(payload))\n",
    "\n",
    "response = response[\"Body\"].read().decode(\"utf-8\")\n",
    "response = json.loads(response)['predictions'][0]\n",
    "\n",
    "source = sentence\n",
    "target = response[\"target\"]\n",
    "attention_matrix = np.array(response[\"matrix\"])\n",
    "\n",
    "print(\"Source: %s \\nTarget: %s\" % (source, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function for plotting the attentioan matrix\n",
    "def plot_matrix(attention_matrix, target, source):\n",
    "    source_tokens = source.split()\n",
    "    target_tokens = target.split()\n",
    "    assert attention_matrix.shape[0] == len(target_tokens)\n",
    "    plt.imshow(attention_matrix.transpose(), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "    plt.xlabel(\"target\")\n",
    "    plt.ylabel(\"source\")\n",
    "    plt.gca().set_xticks([i for i in range(0, len(target_tokens))])\n",
    "    plt.gca().set_yticks([i for i in range(0, len(source_tokens))])\n",
    "    plt.gca().set_xticklabels(target_tokens)\n",
    "    plt.gca().set_yticklabels(source_tokens)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAEYCAYAAADvfWu0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLFsOKDPLlgPKzLL1/z2ONM2WZ3b9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7d7ca86d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(attention_matrix, target, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Protobuf format for inference (Suggested for efficient bulk inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the vocabulary mappings as this mode of inference accepts list of integers and returns list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import tempfile\n",
    "from record_pb2 import Record\n",
    "from create_vocab_proto import vocab_from_json, reverse_vocab, write_recordio, list_to_record_bytes, read_next\n",
    "\n",
    "source = vocab_from_json(\"vocab.src.json\")\n",
    "target = vocab_from_json(\"vocab.trg.json\")\n",
    "\n",
    "source_rev = reverse_vocab(source)\n",
    "target_rev = reverse_vocab(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [\"this is so cool\",\n",
    "            \"i am having dinner .\",\n",
    "            \"i am sitting in an aeroplane .\",\n",
    "            \"come let us go for a long drive .\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the string to integers, followed by protobuf encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert strings to integers using source vocab mapping. Out-of-vocabulary strings are mapped to 1 - the mapping for <unk>\n",
    "sentences = [[source.get(token, 1) for token in sentence.split()] for sentence in sentences]\n",
    "f = io.BytesIO()\n",
    "for sentence in sentences:\n",
    "    record = list_to_record_bytes(sentence, [])\n",
    "    write_recordio(f, record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-recordio-protobuf', \n",
    "                                   Body=f.getvalue())\n",
    "\n",
    "response = response[\"Body\"].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, parse the protobuf response and convert list of integers back to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _parse_proto_response(received_bytes):\n",
    "    output_file = tempfile.NamedTemporaryFile()\n",
    "    output_file.write(received_bytes)\n",
    "    output_file.flush()\n",
    "    target_sentences = []\n",
    "    with open(output_file.name, 'rb') as datum:\n",
    "        next_record = True\n",
    "        while next_record:\n",
    "            next_record = read_next(datum)\n",
    "            if next_record:\n",
    "                rec = Record()\n",
    "                rec.ParseFromString(next_record)\n",
    "                target = list(rec.features[\"target\"].int32_tensor.values)\n",
    "                target_sentences.append(target)\n",
    "            else:\n",
    "                break\n",
    "    return target_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['das ist so cool .', 'ich bin mit dem Abendessen <unk> .', 'ich <unk> Sie in einem Flugzeug .', 'lassen Sie uns eine lange Fahrt gehen .']\n"
     ]
    }
   ],
   "source": [
    "targets = _parse_proto_response(response)\n",
    "resp = [\" \".join([target_rev.get(token, \"<unk>\") for token in sentence]) for\n",
    "                               sentence in targets]\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop / Close the Endpoint (Optional)\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sage.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
